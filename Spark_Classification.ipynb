{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOr7DSN6L/yOr2yJarHNUuC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anastasios-K/Classification/blob/master/Spark_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FR3Miq-ccrhB",
        "colab_type": "code",
        "outputId": "f0789ffa-a25b-4bd2-e51d-c574f59ae171",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (2.4.5)\n",
            "Requirement already satisfied: py4j==0.10.7 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ujdd9FO9Xbqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "import pyspark\n",
        "import os\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import numpy as np\n",
        "from pyspark import StorageLevel\n",
        "from pyspark.mllib.feature import IDF, Normalizer\n",
        "import string\n",
        "from pyspark.mllib.regression import LabeledPoint\n",
        "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, SVMWithSGD\n",
        "import numpy\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Js6f2uN_bOYP",
        "colab_type": "code",
        "outputId": "3e7fe5aa-7dce-4fcf-a9a9-1d73764ba40e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# mount my Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNMso-uFkdCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Update to a specific Java version which is compatible with Pyspark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8wqXloDZYBi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Comment out and run the code below to have a look at the Java version\n",
        "# !java -version # comment out to check the current Java version"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm5N40kkfQgK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/')\n",
        "os.chdir('drive/My Drive/BigData/data/lingspam_public/') # navigate to the data directory\n",
        "dir_list = os.listdir(os.getcwd()) # create a list of the data folders\n",
        "\n",
        "# Comment out and run the code below to have a look at the outcome\n",
        "# print(dir_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deaCWd15DkC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sc = pyspark.SparkContext.getOrCreate() # Initiate Pyspark context"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5P1HBb9eHfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Create the required RDDs depending on the text files \"\"\"\n",
        "\n",
        "def Create_RDDs(directory):\n",
        "    pathstring = os.path.abspath(directory)\n",
        "    tr_RDD = sc.wholeTextFiles(pathstring + \"/part[1-9]/\") # compose a training set using the first 9 folders\n",
        "    ts_RDD = sc.wholeTextFiles(pathstring + \"/part10/\") # keep the last forlder for test set\n",
        "\n",
        "    ts_RDD2 = ts_RDD.map(lambda text: (re.split('[/.]', text[0])[-2], text[1])) \n",
        "    tr_RDD2 = tr_RDD.map(lambda text: (re.split('[/.]', text[0])[-2], text[1]))\n",
        "\n",
        "    ts_RDD2.cache()\n",
        "    tr_RDD2.cache()\n",
        "    pairs = (directory, tr_RDD2, ts_RDD2)\n",
        "    return(pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjawUkxqRLp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pair_list = [Create_RDDs(x) for x in dir_list if \".txt\" not in x] # get the RDDs linked to each directory\n",
        "\n",
        "pair_dict = {pair_list[x][0]:(pair_list[x][1], pair_list[x][2]) for x in range(len(pair_list))} # create a dictionary with the directories and the corresponding RDDs\n",
        "\n",
        "train_Rdd, test_Rdd = pair_dict['bare']\n",
        "\n",
        "partial_Rdd = train_Rdd.sample(True, 0.1, seed=1) # 10% of the train_Rdd to be used in the sections below for facilitation\n",
        "\n",
        "# Comment out and run the code below to have a look at the outcome\n",
        "# WARNING!!! it takes some time\n",
        "\n",
        "# print(train_Rdd.count())\n",
        "# print(test_Rdd.count())\n",
        "# print(train_Rdd.take(1))\n",
        "# print(test_Rdd.take(1))\n",
        "# print(partial_Rdd.take(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GShep7EJBP68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Functions for preprocessing \"\"\"\n",
        "\n",
        "def Tokenisation(text): # Breake down into tokens\n",
        "    nltk.download('punkt')\n",
        "    return(nltk.word_tokenize(text))\n",
        "\n",
        "def Punct_removal(tokens): # Remmove punctuation (the most efficient method)\n",
        "    tokens2 = [token.strip(string.punctuation) for token in tokens]\n",
        "    return(tokens2)\n",
        "\n",
        "def RDD_preparation(rdd): # Implement the entire preprocessing\n",
        "    rdd_vals1 = rdd.values() # Get values ONLY. \n",
        "    rdd_vals2 = rdd_vals1.map(Tokenisation) # Tokenise the values\n",
        "    rdd_vals3 = rdd_vals2.map(Punct_removal) # remove punctuation\n",
        "    rdd_vals4 = rdd.keys().zip(rdd_vals3) # match the kaeys nad values again\n",
        "    RDD_final = rdd_vals4.map(lambda string: (string[0], list(filter(None, string[1])))) # filter out empty strings\n",
        "    return(RDD_final)\n",
        "\n",
        "partial_Rdd1 = RDD_preparation(partial_Rdd) # use the partial_Rdd to test the fucntionality\n",
        "\n",
        "# Comment out and run the code below to have a look at the outcome\n",
        "\n",
        "# print(partial_Rdd1.count())\n",
        "# print(partial_Rdd1.take(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JID497PZrHSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a fixed-size vector from a word list\n",
        "\n",
        "def Hashing_vectors(text,dimensions): # arguments: the list and the size of the output vector\n",
        "    vector = np.zeros(dimensions)  # create vector of 0s\n",
        "    for word in text:\n",
        "        vector[hash(word) % dimensions] += 1 # add 1 at the hashed address \n",
        "    return(vector) # return hashed word vector\n",
        "\n",
        "def Norm_Tfidf(rdd, dimensions):\n",
        "    keys_RDD = rdd.keys()\n",
        "    vals_RDD = rdd.values()\n",
        "\n",
        "    # create vectors\n",
        "    vector_RDD = vals_RDD.map(lambda tokens: Hashing_vectors(tokens, dimensions))\n",
        "    vector_RDD.persist(StorageLevel.MEMORY_ONLY) # save in memeory only to accelarate the process\n",
        "\n",
        "    tfidf_RDD = IDF().fit(vector_RDD).transform(vector_RDD) # implement TF.IDF algorithm\n",
        "    norm_tfidf_RDD = Normalizer().transform(tfidf_RDD) # round the hashing values to 1\n",
        "    final_tfidf_RDD = keys_RDD.zip(norm_tfidf_RDD) # match the corresponding rdd keys\n",
        "    return(final_tfidf_RDD)\n",
        "    \n",
        "\n",
        "dimensions = 20 # use low dimensionality value to test\n",
        "partial_rdd2 = Norm_Tfidf(partial_Rdd1, dimensions) # use the latest partial_Rdd(1) to check the functionality\n",
        "\n",
        "# Comment out and run the code below to have a look at the outcome\n",
        "# print(partial_rdd2.take(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kBoieF1Ti9w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8a8cb7f2-1048-433a-dc48-940f91a9293c"
      },
      "source": [
        "\"\"\" For user to get a visual representation of the RDD \"\"\"\n",
        "# comment out and run the code below to get a line by line visual representation\n",
        "\n",
        "# rdd_sample = partial_rdd2.take(100)\n",
        "# for x in rdd_sample:\n",
        "#   print(x,\"\\n\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' For user to get a visual representation of the RDD '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNhdZhI5epVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate the target labels (whether it is SPAM or NOT)\n",
        "# NOTE: 1 --> SPAM    &    0 --> NO SPAM\n",
        "\n",
        "def Create_Labels(rdd): \n",
        "    class_vector_RDD = rdd.map(lambda label: (1 if (label[0].startswith('spmsg')) else 0, label[1])) # detect spam based on \"spmsg\" text at the beginning\n",
        "    target_RDD = class_vector_RDD.map(lambda cv: LabeledPoint(cv[0],cv[1]) ) \n",
        "    return(target_RDD)\n",
        "\n",
        "final_partial = Create_Labels(partial_rdd2)\n",
        "\n",
        "# Comment out and run the code below to have a look at the outcome\n",
        "# print(final_partial.take(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3U-R_BJyzna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Train and Test 2 algorithms \"\"\"\n",
        "\n",
        "def Train_Model(train_rdd):\n",
        "    starting_point = time.perf_counter()\n",
        "    print('Training process')\n",
        "    print(\"-\" * 80)\n",
        "    model1 = LogisticRegressionWithLBFGS.train(train_rdd) # logistic regression\n",
        "    print(\"Model 1 --> Logistic Regression\")\n",
        "    print(f\"{type(model1)}\")\n",
        "    print(\"-\" * 80)\n",
        "    model2 = SVMWithSGD.train(train_rdd) # support vector machine\n",
        "    print(\"Model 2 --> SVM\")\n",
        "    print(f\"{type(model2)}\")\n",
        "    print(\"-\" * 80)\n",
        "    end = time.perf_counter()\n",
        "    print(\"\\n\", f\"Execution time --> {round(end - starting_point, 2)} in sec\") # counting the execution time\n",
        "    return (model1,model2)\n",
        "\n",
        "def Test_Model(model, test_rdd):\n",
        "    pred_and_targ = test_rdd.map(lambda element: (model.predict(element.features), element.label)) # get the prediction and ground truth (label) for each item.\n",
        "    correct = pred_and_targ.filter(lambda label: label[0] == label[1]).count() # count the correct predictions \n",
        "    accuracy = correct / pred_and_targ.count()\n",
        "    print(f\"--> {accuracy} (data items: {pred_and_targ.count()}, correct: {correct})\")\n",
        "    return(accuracy)\n",
        "\n",
        "# Comment out to check (final_partial is used once more to easily test the functionality)\n",
        "\n",
        "# Log_Reg, SVM = Train_Model(final_partial)\n",
        "# accur_evaluation = Test_Model(SVM, final_partial)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH8qua1wp6Cv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Apply all the above to the actual Training RDD (\"Train_Rdd) which is created earlier \"\"\"\n",
        "\n",
        "def Preproc_Full(rdd,dimensions): # the functions below have been fully explained earlier\n",
        "    train_Rdd1 = RDD_preparation(rdd)\n",
        "    train_Rdd2 = Norm_Tfidf(train_Rdd1, dimensions)\n",
        "    train_Rdd3 = Create_Labels(train_Rdd2)\n",
        "    return(train_Rdd3)\n",
        "\n",
        "new_dimensions = dimensions # dimensionality value may change BUT currently the previous number is used\n",
        "final_TRAIN_rdd = Preproc_Full(train_Rdd, new_dimensions)\n",
        "final_TEST_rdd = Preproc_Full(test_Rdd, new_dimensions)\n",
        "\n",
        "# print(final_TRAIN_rdd.take(1))\n",
        "# print(final_TEST_rdd.take(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0j9_MDIjv9h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "3a7de2bc-8ba4-4903-fc5d-536bc52ab73a"
      },
      "source": [
        "def Train_Test_preocess(train_rdd,test_rdd):\n",
        "    models = Train_Model(train_rdd)\n",
        "    results = [[],[]] # matrix for 2 modes (training/test) vs n models (currently 3)\n",
        "    for x, model in enumerate(models):\n",
        "        print(\"\\n\", f\"Model {x+1}\")\n",
        "        print('Training Accuracy')\n",
        "        results[0].append(Test_Model(model, train_rdd))\n",
        "        print(\"-\" * 80)\n",
        "        print('Test Accuracy')\n",
        "        results[1].append(Test_Model(model, test_rdd))\n",
        "    return(results)\n",
        "\n",
        "model_evaluation = Train_Test_preocess(final_TRAIN_rdd, final_TEST_rdd)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training process\n",
            "--------------------------------------------------------------------------------\n",
            "Model 1 --> Logistic Regression\n",
            "<class 'pyspark.mllib.classification.LogisticRegressionModel'>\n",
            "--------------------------------------------------------------------------------\n",
            "Model 2 --> SVM\n",
            "<class 'pyspark.mllib.classification.SVMModel'>\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            " Execution time --> 78.67 in sec\n",
            "\n",
            " Model 1\n",
            "Training Accuracy\n",
            "--> 0.9062259800153728 (data items: 2602, correct: 2358)\n",
            "--------------------------------------------------------------------------------\n",
            "Test Accuracy\n",
            "--> 0.7285223367697594 (data items: 291, correct: 212)\n",
            "\n",
            " Model 2\n",
            "Training Accuracy\n",
            "--> 0.8339738662567256 (data items: 2602, correct: 2170)\n",
            "--------------------------------------------------------------------------------\n",
            "Test Accuracy\n",
            "--> 0.8316151202749141 (data items: 291, correct: 242)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}